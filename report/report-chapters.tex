\section{Introduction and Motivation}
\label{sec:introduction}
Twitter, as any other social network and blog, is a constant source of unstructured data that, if processed in the right way, can be leveraged to obtain valuable insights in several fields. Due to this reason, lots of companies have started to collect this huge amount of data in order to perform in-depth analysis on it; many of them have initiated to perform sentiment analysis to check customers satisfaction with respect to their products. Other organizations have started to use this asset, hereafter called Big Data, in their decision-making process that includes reporting, exploration of data and exploratory search (e.g., finding correlations). According to upGrad \cite{upGrad}, Big Data can help create pioneering breakthroughs for companies that know how to use it correctly.

A challenging aspect in Big Data analysis is to extract valuable insights that can drive business strategies and decisions. In this context, the social network Twitter plays a crucial role during the information retrieval phase: millions of people publish everyday short messages, hereafter tweets, that often contain their desires or demands. The aim of many companies is to leverage these tweets in order to catch common issues or demands to guide their business strategies for their products or services. These insights, according to upGrad \cite{upGrad}, can be used to develop new products/services, enhance marketing techniques, optimize customer service, improve employee productivity and find radical ways to expand brand outreach.

In addition, organizations aim also to intercept occasional issues or demands that are not constant over time but are very popular in a bounded period of time. This is the case when there is a sudden issue or request for a particular product or service that affects many customers. Catching this kind of insights, permits companies to better address their business short-term decision-making phase and enhance promptly their products or services. According to Ben Ridler \cite{customer-satistaction}, a business owner ability to effectively deal with customer complaints provides a great opportunity to turn dissatisfied customers into active promoters of the business.

Nowadays, according to \cite{twitter-stats}, there are more than 500 millions new tweets each day and this statistic is still growing every year. Thanks to this fact, many people have started to collect and group those tweets into datasets; many of them are easily and freely accessible by anyone through internet. Despite the fact that many of those datasets contain noisy data that force people to pre-process them, companies can elaborate those tweets in order to achieve the above mentioned business goals.

The aim of this project is to address in an efficient and effective manner the procedure of finding consistent topics over time inside tweet texts. In other words, the goal is to find topics that are frequent locally in a specific time frame but those topics, in order to be considered, must be frequent locally in a sufficient number of time frames. In this report are described in detail the solution to the just mentioned problem and the relative implementation. Finally, in Section \ref{sec:experimental_evaluation}, is presented an experimental evaluation of this project using the dataset described in Section \ref{sec:problem_statement} and \ref{sec:dataset} \cite{covid19-tweets-dataset}.

\section{Related work}
\label{sec:related_word}
In order to design and implement this project, several methods and techniques have been identified and implemented; anyway, there are some of them that have been implemented and widely tested but then discarded since they have not produced acceptable results. In the first part of this section are described all those techniques used in order to pre-process the dataset, while in the second part are described all those methods used to find consistent topics over time. 

During the pre-processing phase of the dataset have been adopted several techniques used in natural language processing scenarios. The methods that have been implemented and kept in the final preprocessor script are described in details below and applied following this ordering:
\begin{itemize}
	\item \textbf{Sentence segmentation}: the text string of each tweet is divided into sentences. In other words, the result of this step is a list composed by all the sentences written inside the tweet;
	\item \textbf{Tokenization}: with this technique, each sentence obtained from the previous step is split into even smaller parts called \textit{tokens}. In particular, has been used the blank character as string separator: due to this reason, a token corresponds to a word of the sentence;
	\item \textbf{POS Tagging}: this is the most complex task during the pre-processing of the dataset. The idea of the POS (\textit{Part Of Speech}) technique is to assign to each token the corresponding tag (noun, verbs, adjectives, conjunction, etc.). The collection of tags used (tag set) can be considered as standard since it is used widely in the natural language processing community. In order to implement this technique, the map provided by the Python nltk library \cite{python-nltk} has been used.
\end{itemize}
The other technique used during the pre-processing of the dataset is an entity resolution method that is inspired to \textit{lemmatisation} \cite{lemmatisation-stemming}. The just mentioned technique aims to reduce the inflected form of a word in its canonical form in order to group together different words that have the same canonical form. The process implemented in this project is a simpler and non-automatic version of lemmatisation but still very effective on the given dataset. The goal is to define a set of \textbf{aliases} stored on the disk in a CSV file interpreted as a dictionary: if a word has an entry in that data structure, it will be replaced with its alias that represents a more general word (see Section \ref{sec:dataset} for further details).

\noindent Another technique that was initially implemented is \textbf{stemming} \cite{lemmatisation-stemming}. The aim of this method is to reduce the inflected form of a word in its root form (or "word stem") in order to group together words that have the same root form. The idea was to substitute the given word with its stem during the pre-processing phase. The major problem arose when the script to find the consistent topics over time has been run: the obtained results were not so meaningful since the root form of a word cannot be presented as a topic to the final user. Due to this reason and after many attempt to improve this approach, the just described technique has been abandoned.

In order to find out all the consistent topics over time, the major technique that has been used is the \textbf{frequent itemsets} mining method. The goal of this technique is to identify all the sets of items (in the context of this project, set of words) that appears together a sufficient number of times. In order to achieve the goal of this project, the association rules are not computed since they do not produce any valuable information. In a first attempt, the \textbf{A-Priori} algorithm \cite{apriori-algorithm} was used and implemented. The goal of this procedure is to find out initially frequent individual terms in the dataset and then extending the scope to larger item sets as long as those item sets appear sufficiently often in the database. The A-Priori algorithm leverages the contra-positive \textit{monotonicity} principle: if a word \textit{s} does not appear in \textit{N} sets, then no pair that includes \textit{s} can appear in \textit{N} sets.

\noindent A further attempt was to implement the PCY algorithm or its multihash variation \cite{pcy-multihash-algorithms}, but a more efficient technique that can be parallelized between many machines has been found: \textbf{FP-Growth} \cite{fpgrowth-paper}. The first step of this procedure, similarly to the A-Priori one, is to compute item frequencies and identify frequent items. Then, the second step of FP-Growth is to use a suffix tree structure, called FP-tree, to encode transactions without generating candidate sets explicitly, due to the fact that they are usually expensive to generate. When this second phase terminates the execution, the computed frequent itemsets can be extracted from the FP-tree. With the aim of speed-up the computation distributing the workload among different machines, a parallelizable version of FP-Growth has been used: \textbf{Parallel FP-Growth} \cite{pfpgrowth-paper}. This algorithm distributes the work of growing FP-trees based on the suffixes of transactions. Due to this improvement, it is more scalable than a single-machine computation.

\noindent The last technique used with the aim to improve the final result presented to the user is a variation of the \textbf{closed itemsets} method. The just mentioned technique must be executed after having identified all the frequent itemsets (when the PFP-Growth algorithm has terminated its execution) and its goal is to compact the final output. The idea of closed itemsets is that if there are two frequent itemsets where one is a subset of the other and they have the same frequency, the subset must be discarded. The method implemented in this project follows the same principle, but given two itemsets where one is a subset of the other, the subset is discarded only if it appears in as many time frames as the super set. If they do not appear in the same number of time frames, it means that probably they are different topics so they must be both kept in the result set. The frequency of appearance of the itemsets is not used as a discriminant parameter in order to decide whether to keep or not a subset of items due to the fact that, in any case, a topic composed by many items is much more meaningful than a topic with less items to the final user.

\section{Problem statement}
\label{sec:problem_statement}
In order to achieve the goal described in Section \ref{sec:introduction} (i.e., identify consistent topics over time inside tweets), a public available dataset that groups more than 300.000 tweets that contain the hashtag \textit{\#covid19} \cite{covid19-tweets-dataset} has been used. As many other datasets composed of unstructured data dumped from a social network such as Twitter, this dataset has lots of fields that characterize each tweet (e.g., publisher's username, location and account information, text of the message, etc.). 

To identify consistent topics over time using those tweets, the only fields needed are the publication date of each of them and the relative text. In other words, in the working environment a tweet is defined as a tuple composed by the following fields:
\begin{itemize}
	\item \textbf{date}: the date of the tweet publication, expressed as an integer value that represents the number of seconds that have elapsed since the midnight of 1\textsuperscript{st} January 1970;
	\item \textbf{text}: the text of the tweet represented as a list of words. This list is obtained splitting the source text using the blank character " " as separator and processed as described in detail in Section \ref{sec:dataset}.
\end{itemize}

Therefore, in order to obtain the formal model of this problem, the following sets are been defined:
\begin{description}
	\item[TWEET] = the set of tweets, defined as $\mathrm{TIMESTAMP} \times \mathrm{TEXT}$ where $\mathrm{TIMESTAMP} \subset \mathbb{N}_{\geq 0}$ and $\mathrm{TEXT} = \{x | x \in \Sigma^*\}$ where $\Sigma$ is the reference alphabet;
	\item[TIMESPAN] = the set of all the possible time spans in which the input dataset can be split, defined as a subset of $\mathbb{N}_{\geq 0}$
	\item[TS-THRESHOLD] = the set of all the possible thresholds to identify frequent terms and topics in a single time span, defined as a subset of $\mathbb{N}_{\geq 0}$
	\item[GB-THRESHOLD] = the set of all the possible thresholds to identify consistent topics over all the time spans, defined as a subset of $\mathbb{N}$
\end{description}

\noindent The aim is to model an utility function \textit{f} defined as
\begin{center}
	\textit{f}: $\mathrm{INPUT} \mapsto \mathrm{TOPIC}$
\end{center}
where $\mathrm{INPUT}$ is defined as 
\begin{center}
	$\mathrm{TWEET} \times \mathrm{TIMESPAN} \times \mathrm{TS\-THRESHOLD} \times \mathrm{GB\-THRESHOLD}$
\end{center}
 and $\mathrm{TOPIC} = \{x | x \in \Sigma^*\}$ where $\Sigma$ is the reference alphabet. In other words, $\mathrm{TOPIC}$ is the set of all the possible consistent topics over all the identified time spans.

\section{Solution}
\label{sec:solution}


\section{Implementation}
\label{sec:implementation}


\section{Dataset}
\label{sec:dataset}
As described previously in Section \ref{sec:problem_statement}, in order to build and test the solution presented in Section \ref{sec:solution} a public available CSV dataset with more than 300.000 tweets that contain the hashtag \textit{\#covid19} \cite{covid19-tweets-dataset} has been downloaded from kaggle.com and processed. As many other datasets that group together huge quantity of unstructured data, it had to undergo a pre-processing phase in order to remove unnecessary features and noisy data. 

In that dataset, a tweet is defined as a data item composed of thirteen features: \textit{user-name}, \textit{user-location}, \textit{user-description}, \textit{user-created}, \textit{user-followers}, \textit{users-friends}, \textit{user-favourites}, \textit{user-verified}, \textit{date}, \textit{text}, \textit{hashtags}, \textit{source}, \textit{is-retweet}. In order to achieve the goal described in Section \ref{sec:problem_statement}, only two features are needed: the publication date of the tweet and the relative text. Due to this reason, the first stage of the pre-processing consists to cut off all the non-relevant data features. This has been achieved quite easily since the dataset is CSV (Comma Separated Value) file and the Pandas Python library \cite{python-pandas} has lots of APIs to manage efficiently such big datasets.

After this initial stage, each tweet has been processed singularly in a multi-process execution where to each process is assigned a portion of the input dataset. For each tweet in the dataset portion, each process has to manipulate both the date and the text fields in the following manner:
\begin{description}
	\item[Date] field is transformed into an UNIX timestamp. In particular the publication date is stored in the input dataset as a string formatted as "yyyy-mm-dd hh:mm:ss", but thanks to the \textit{datetime} Python library this transformation can be performed in a easy and fast way. With the aim to be complaint also with the format of another dataset that groups together tweets in order to perform sentiment analysis \cite{sentiment-analysis-dataset}, the script responsible for the date manipulation is capable to transform also dates formatted like "Mon Apr 06 22:19:45 PDT 2009" into a UNIX timestamp.
	\item[Text] field is transformed into a list of words. In particular, thanks to the \textit{nltk} \cite{python-nltk} Python library, the initial text has been split using the blank character " " as separator and only the useful words that can be leveraged in order to derive a topic have been kept. Due to this reason, only nouns and adjectives appear in the final result list. Furthermore, there are other situations where a word, even it is a name or an adjective, cannot be considered: 
	\begin{itemize}
		\item if the word contains a slash "/", non-ASCII characters, special Unicode sequences;
		\item if the word represents a numeric value;
		\item if the word is composed by only one character;
		\item if the word is a stop word. This method has been implemented because there are some words like "https" or "amp" that are tagged as nouns but they cannot be leveraged to build a topic.
	\end{itemize}
	As a note, if the preprocessor script has been executed in the debug mode, all the non-considerable words are dumped inside a separated CSV file (together with the relative timestamp) in order to check if the script cut off useful words. Then, each considerable word is filtered and sanitized in order to remove all the eventual noisy characters. In particular, the filtering method performs the following operations:
	\begin{enumerate}
		\item all the word's characters are lowered;
		\item all the emojis are removed;
		\item all the non-alphanumeric characters are substituted with a blank space (e.g., the word "white-house" is transformed to "white house");
		\item the word is split again using the blank character as separator to identify eventual other words after the previous operation;
		\item all the obtained words are rechecked again to see if they are considerable and, eventually, discarded;
		\item as a final operation, each of the obtained words are checked with an aliases map. If there is an entry for a word, then the associated value is substituted. The aim is to generalize as much as possible the words used inside the tweets leveraging the knowledge about that dataset in order to obtain better results: since all the texts contain the word "covid19" or one of its many variations (e.g., "sars-cov-2", "covid", "coronavirus", etc.), all of them are mapped to the word "covid19".
	\end{enumerate}
	After these filtering operations, a final check is performed over all the considerable words in order to find out if there are doubled words or empty string. 
\end{description}

When each process has terminated its execution, all the partial results are collected in a single list by the master process using a pipe-based communication and sort that list in ascending order using the timestamp field as key. The result is a list of tweets ordered by their publication: this feature can be leveraged by the main script described in Section \ref{sec:implementation} in order to lower the overall complexity. If the preprocessor script has been executed in debug mode, also the list that contains all the discarded words would be collected but not sorted.

As a final step, the result list has been converted by the master process in a Pandas \cite{python-pandas} dataframe object and stored in a separated CSV file. The same procedure would be applied to the second list with all the discarded words if the preprocessor script has been executed in debug mode.

\section{Experimental evaluation}
\label{sec:experimental_evaluation}

\subsection{Test dataset}
\label{subsec:test_dataset}
